#! bin/bash
python fine_prune_gpt2.py --output_dir /home/aobaruwa/codebase/Pruning/out \
                          --log_dir /home/aobaruwa/codebase/logs/pruning \
                          --train_file /home/aobaruwa/codebase/dd_data/train.256len.db \
                          --eval_file /home/aobaruwa/codebase/dd_data/val.txt \
                          --model_type masked_gpt2 \
                          --model_path /home/aobaruwa/codebase/model/medium_ft.pkl \
                          --ckpt_file /home/aobaruwa/codebase/model/medium_ft.pkl \
                          --per_gpu_train_batch_size 4 \
                          --grad_acc_steps 8 \
                          --val_step 600 \
                          --max_steps 3000 \
                          --max_seq_len 150 \
                          --warmup_steps 100 \
                          --initial_warmup 1 \
                          --final_warmup 1 \
                          --lr 5e-5 \
                          --local_rank -1 \
                          --pruning_method l0 \
                          --mask_init constant \
                          --mask_scores_learning_rate 1e-1 \
                          --mask_scale 2.197 \
                          --regularization l0 \
                          --final_lambda 125. \
                          --opt_level O2 \
                          --use_fp16